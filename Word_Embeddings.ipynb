{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danalmestadi/T5-Week-seven/blob/main/Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Word Embeddings\n",
        "\n",
        "Word embeddings are a type of word representation that allows words to be represented as continuous vectors in a high-dimensional space. Unlike traditional representations like Bag of Words (BoW), word embeddings capture semantic meanings and relationships between words by placing similar words closer together in the vector space.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Word Embedding**: A dense vector representation of a word where each dimension captures some aspect of its meaning.\n",
        "2. **Pre-trained Embeddings**: Embeddings learned from large corpora, such as Word2Vec, GloVe, and FastText.\n",
        "3. **Semantic Similarity**: Words with similar meanings will have similar embeddings, making it easier to perform tasks like word similarity and analogy."
      ],
      "metadata": {
        "id": "ihBopOobLb2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n"
      ],
      "metadata": {
        "id": "J14EKzDQLg2f"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Load pre-trained Word2Vec model (Google News vectors)\n",
        "Note: This model is quite large. For demonstration, use a smaller or different model as needed.\n",
        " model = KeyedVectors.load_word2vec_format('path/to/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "6YBv0QyqLiSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For demonstration, we'll use a smaller pre-trained model available in gensim\n",
        "from gensim.downloader import load\n",
        "model = load('glove-wiki-gigaword-50')"
      ],
      "metadata": {
        "id": "Iyp8HlpbLmHl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82c72584-2780-4c4a-b7a8-886b42dd9f49"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example words\n",
        "words = ['school', 'bus', 'kid', 'car']"
      ],
      "metadata": {
        "id": "xkigRBgHLoiP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get embeddings\n",
        "embeddings = {word: model[word] for word in words}"
      ],
      "metadata": {
        "id": "xJngiE2qLqOr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H01quYFFLWE0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38531ce8-30e7-442c-f27e-e7294cc9dd5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: school\n",
            "Embedding: [-0.90629    1.2485    -0.79692   -1.4027    -0.038458  -0.25177\n",
            " -1.2838    -0.58413   -0.11179   -0.56908   -0.34842   -0.39626\n",
            " -0.0090178 -1.0691    -0.35368   -0.052826  -0.37056    1.0931\n",
            " -0.19205    0.44648    0.45169    0.72104   -0.61103    0.6315\n",
            " -0.49044   -1.7517     0.055979  -0.52281   -1.0248    -0.89142\n",
            "  3.0695     0.14483   -0.13938   -1.3907     1.2123     0.40173\n",
            "  0.4171     0.27364    0.98673    0.027599  -0.8724    -0.51648\n",
            " -0.30662    0.37784    0.016734   0.23813    0.49411   -0.56643\n",
            " -0.18744    0.62809  ]\n",
            "\n",
            "Word: bus\n",
            "Embedding: [ 0.84772   0.070253  0.96791  -0.27164  -0.37617   0.31978  -1.3108\n",
            "  0.091093  0.59919  -0.90217  -0.050876 -0.83886  -0.61596   0.29642\n",
            " -0.42189  -0.21969  -0.94006   1.2221   -0.66526  -0.57745   0.76126\n",
            "  0.51459  -0.88565   1.5135    0.42326  -1.2947    0.45522   0.67073\n",
            "  0.80188  -0.65449   2.4117    0.62445  -0.046631  0.37524   1.0103\n",
            "  0.25259   1.0913   -0.79427  -0.17027   1.4866   -0.24077   0.021904\n",
            " -0.01603  -0.44319  -0.13914   0.013311 -0.49432  -0.57696   1.1997\n",
            " -0.39707 ]\n",
            "\n",
            "Word: kid\n",
            "Embedding: [-8.0514e-01  1.9169e-02  4.3794e-02 -3.4736e-01  8.7249e-01 -1.7453e-02\n",
            " -9.1692e-01 -2.0132e-01 -3.8583e-01  1.0267e+00 -1.3576e-01  3.5118e-01\n",
            " -5.6127e-02  3.1572e-01  6.6075e-01 -1.2189e-02  1.0945e-01  1.4896e+00\n",
            " -2.9475e-01  7.5500e-03 -7.6327e-01  7.5765e-01  9.9607e-02  1.0643e+00\n",
            "  5.2815e-01 -1.5519e+00 -9.3494e-01  3.2375e-01  1.1516e+00 -1.1297e+00\n",
            "  1.5927e+00  2.7941e-01 -2.9508e-01  3.7832e-01  4.5020e-01  4.1688e-01\n",
            " -2.2306e-01 -3.8199e-01  4.7368e-01 -1.3568e+00 -4.0257e-01  1.3353e-01\n",
            " -8.3670e-01  9.7634e-02  5.8337e-01 -1.2157e-03  7.2061e-01 -9.4734e-01\n",
            "  1.1507e-02  1.0481e+00]\n",
            "\n",
            "Word: car\n",
            "Embedding: [ 0.47685  -0.084552  1.4641    0.047017  0.14686   0.5082   -1.2228\n",
            " -0.22607   0.19306  -0.29756   0.20599  -0.71284  -1.6288    0.17096\n",
            "  0.74797  -0.061943 -0.65766   1.3786   -0.68043  -1.7551    0.58319\n",
            "  0.25157  -1.2114    0.81343   0.094825 -1.6819   -0.64498   0.6322\n",
            "  1.1211    0.16112   2.5379    0.24852  -0.26816   0.32818   1.2916\n",
            "  0.23548   0.61465  -0.1344   -0.13237   0.27398  -0.11821   0.1354\n",
            "  0.074306 -0.61951   0.45472  -0.30318  -0.21883  -0.56054   1.1177\n",
            " -0.36595 ]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display embeddings\n",
        "for word, vector in embeddings.items():\n",
        "    print(f\"Word: {word}\\nEmbedding: {vector}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find similar words\n",
        "similar_words = model.most_similar('mobile', topn=5)\n",
        "\n",
        "# TODO:: Display similar words\n",
        "similar_words"
      ],
      "metadata": {
        "id": "rKCazO0NLsOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8150824b-1354-4f37-b96f-d26c1c7502d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('wireless', 0.8815027475357056),\n",
              " ('phones', 0.805967390537262),\n",
              " ('broadband', 0.7867703437805176),\n",
              " ('cellular', 0.7820915579795837),\n",
              " ('operating', 0.7751185297966003)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solve analogy\n",
        "analogy_result = model.most_similar(positive=['teacher', 'student'], negative=['school'], topn=1)\n",
        "\n",
        "# TODO:: Display result\n",
        "analogy_result"
      ],
      "metadata": {
        "id": "fkcpJPKvL43C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b52aa5c-0b84-4ac9-e028-0ba57cb557a2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('educator', 0.7146477699279785)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}